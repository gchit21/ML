**"House Prices - Advanced Regression Techniques"**
	კონკურსის მოკლე მიმოხილვა: კონკურსის მთავარი მიზანია დავატრეინინგოთ მოდელი, რომელიც ძალიან დაბალი error-ით, დაგვიდგენს
    მოცემული სახლის გასაყიდ(სარეალიზაციო) ფასს. Data set-ში გვაქვს 1460 data entry და 80-მდე Feature, რომელთა უმეტესობა კატეგორიული 
    ცვლადია და საჭიროებს რიცხვითში გადაყვანას. გადაყვანის შემდეგ საჭირო იქნება ფილტრაცია და feature-ების სწორი შერჩევა, რათა ჩვენმა
    მოდელმა დადოს ოპტიმალური შედეგი. შემფასებელი მეტრიკა RMSLE (root mean square log error), რაც უფრო ახლოსაა 0-თან მით უკეთესი
		
**რეპოზიტორიის სტრუქტურა:**
	რეპოზიტორია შეიცავს 3 ფაილეს: README ფაილი(ამ წამს კითხულობ ამას), model_experiment.ipynb - პითონის ფაილი, სადაც არის Data set-ის
    cleaning, engineering,correlation filtering,scaling, feature selection და მოდელების
    გაწვრთნის ნაწილი(რამდენიმე მოდელია, linear, lasso, rigid). model_inference.ipynb-კი არის ჩემი საუკეთესო მოდელი, რისი გაწვრთნაც 
	შევძელი (ჩვეულებრივი LinearRegression model არის, თუმცა Lasso და Rigid-საც მსგავსი მეტრიკები ჰქონდათ). მოდელის იმპორტის კოდი ბოლო
	cell-ში წერია, ხოლო მანამდე კოდი test.csv ფაილის იმ სახემდე მიყვანას ემსახურება, ჩემმა მოდელმა რომ მიიღოს
	

**Feature Engineering: **
	მიდგომა მქონდა შემდეგი: პირველ რიგში ვნახე რა feature-ებს ჰქონდათ "NA" მნიშვნელობები ძალიან მაღალი პროცენტულობით.შედეგად გადავყარე
	მასეთი კატეგორიული ცვლადები,რადგან არანაირ მნიშვნელოვან ინფოს არ მაძლევდნენ. ასევე გადავყარე ისეთი numerical feature-ები,რომელთაც
	95%+ 0-ები ჰქონდათ ველებში, იმავე მიზეზით და თან over-fitting-ს ავარიდე თავი ამათი გადაგდებით. შემდეგ საჭირო იყო დარჩენილი
	feature-ების "NA" ველების შევსება, კატეგორიული მოდით შევავსე, რიცხვითი საშუალოთი. ამის შემდეგ ჯერ WOE encoding-ით
	გადავიყვანე ისეთი კატეგორიული ცვლადები რიცხვითში, რომელთაც 3-ის ტოლი ან მეტი მნიშვნელობის მიღება შეეძლოთ და შემდეგ
	one-hot Encoding-ით გადავიყვანე რა კატეგორიული ცვლადებიც დარჩა. feature-ები რომელთაც მაღალი კორელაცია ჰქონდათ გადავარჩიე 
	და გადავაგდე, threshold-ად ავიღე 0.65(ექსპერიმენტულად მივიღე, 0.4-0.7 რეინჯში ვეძებდი და ამ მნიშვნელობაზე მივიღე საუკეთესო შედეგი).
	
**Feature Selection:**
    feature selection-სთვის გამოვიყენე RFE, რომელსაც n_features_selected არგუმენტს ვუცვლიდი და სხვადასხვა მნიშვნელობაზე 
	ვტესტავდი(ძირითადად 5-20 რეინჯში). თითოეულ მოდელს საკუთარი RFE აქვს და თან ვ-log-ავ იმ feature-ებს, რომელთაც 
	იყენებს.

**Training:**
	ტრეინინგის დროს გამოვიყენე SKlearn ბიბლიოთეკის LinearRegression-ი და LogisticRegression(ეს იმდენად ცუდი იყო რომ წავშალე). ამათ გარდა გამოვიყენე lasso და rigid,
	თუმცა დიდი განსხვავება არ აქვთ. შესაფასებლად გამოვიყენე R2, MAPE(mean absolute percanta),RMSE(root mean square error), 
	RMSLE(root mean square log error - ამას წესით თავად competition-ი იყენებს შესაფასებლად).ვალიდაციისთვის KFold -არ გამოვიყენე, 
	უბრალოდ train.csv დავყავი train და test ნაწილად. დატესტვის შემდეგ ვნახე, რომ დიდი განსხვავება არ იყო linear-ს, lasso-სა 
	და rigid-ს შორის. საბოლოოდ რომ ავტვირთე პასუხი დაბრუნებული  

**MLflow Tracking:**
	MLflow ექსპერიმენტების ბმული: https://dagshub.com/gchit21/ML.mlflow
	ჩაწერილი მეტრიკები: R^2,MAPE(mean_absolute_percentage_error),
	საუკეთესო მოდელის მეტრიკები: split-ზე: R2:0.8376537244766863, RMSLE:0.1834379952481455, MAPE:0.13064341569114407, RMSE:35288
								   პასუხი competition-ზე: RMSLE:0.191 overfitting არის Train-ზე(ლოგიკურიცაა outlier წერტილები არ ამომიკლია)

